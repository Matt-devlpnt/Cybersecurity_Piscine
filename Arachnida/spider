#!/usr/bin/env python3


##########
# Import #
##########

import os
import sys
import argparse
import threading
import signal
import requests
import warnings
from urllib.parse import urljoin, urlparse
from collections import defaultdict
from pathlib import Path
from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning
from PIL import Image, UnidentifiedImageError

####################
# Globale variable #
####################

# Error messages colors
BG_CTRL_C = "\033[48;5;79m"
BG_INFO = "\033[48;5;195m"
BG_PB = "\033[48;5;9m"
BG_TIMEOUT = "\033[48;5;69m"
BG_REDIRECT = "\033[48;5;192m"
BG_CONNEXION = "\033[48;5;222m"
BG_HTTP = "\033[48;5;141m"
BG_UNKNOWN = "\033[48;5;214m"

FG_BG_CTRL_C = "\033[1;38;5;236m"
FG_BG_INFO = "\033[1;38;5;236m"
FG_BG_PB = "\033[1;38;5;236m"
FG_BG_TIMEOUT = "\033[1;38;5;236m"
FG_BG_REDIRECT = "\033[1;38;5;236m"
FG_BG_CONNEXION = "\033[1;38;5;236m"
FG_BG_HTTP = "\033[1;38;5;236m"
FG_BG_UNKNOWN = "\033[1;38;5;236m"

FG_CTRL_C = "\033[38;5;79m"
FG_INFO = "\033[38;5;195m"
FG_PB = "\033[38;5;9m"
FG_TIMEOUT = "\033[38;5;69m"
FG_REDIRECT = "\033[38;5;192m"
FG_CONNEXION = "\033[38;5;222m"
FG_HTTP = "\033[38;5;141m"
FG_UNKNOWN = "\033[38;5;214m"

RESET_COLOR = "\033[0m"


# Files extension handled
EXTENSION_FILE = (
    '.jpg',
    '.jpeg',
    '.png',
    '.gif',
    '.bmp'
)

# html markup and properties handled
BALISE_ELEMENT = {
    'a': 'href',
    'link': 'href',
    'iframe': 'src',
    'img': 'src',
    'source': 'srcset'
}

###########
# Threads #
###########

# Mutex
lock_check_url = threading.Lock()

# Stop event handler for threads
stop_event = threading.Event()

##########
# System #
##########

# disable ^C on terminal for ctrl+C
os.system("stty -echoctl")

###########
# Program #
###########

# This function handle ctrl+C signal
def signal_handler(sig, frame):

    stop_event.set()
    print(f"{BG_CTRL_C}{FG_BG_CTRL_C}[Ctrl+C]   {RESET_COLOR}{FG_CTRL_C} JUST WAIT ENDING OF THREADS PLEASE{RESET_COLOR}")

signal.signal(signal.SIGINT, signal_handler)


# This function handle starting parsing
def parse_args():

    parser = argparse.ArgumentParser(description='Spider for downloading images from a URL')

    parser.add_argument('URL', help='The URL of the site to scrape')
    parser.add_argument('-r', action='store_true', \
        help='Recursively downloads the images in a URL received as a parameter.')
    parser.add_argument('-l', metavar='[N]', type=int, default=5, \
        help='Indicates the maximum depth level of the recursive download. If not indicated, it will be 5.')
    parser.add_argument('-p', metavar='[PATH]', default='./data/', \
        help='Indicates the path where the downloaded files will be saved. If not specified, ./data/ will be used.')

    return parser.parse_args()


# This function check if the image get unique name
def unique_filepath(directory, filename):

    base, ext = os.path.splitext(filename)
    counter = 1
    new_filename = f"{base[:20]}_{counter}{ext}"

    while os.path.exists(os.path.join(directory, new_filename)):
        new_filename = f"{base[:20]}_{counter}{ext}"
        counter += 1

    return os.path.join(directory, new_filename)


#This function check if image file exist and if image is a real image
def image_validator(filepath):

    if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:
        if os.path.exists(filepath):
            os.remove(filepath)
        return False

    try:
        with Image.open(filepath) as img:
            img.verify()
    except (UnidentifiedImageError, OSError, Exception):
        if os.path.exists(filepath):
            os.remove(filepath)
        print(f"{BG_INFO}{FG_BG_INFO}[INFO]     {RESET_COLOR}{FG_INFO} {filepath} removed (invalid image){RESET_COLOR}")
        return False

    return True


# This function makes two action, write images or stock address for next time
def get_page_actions(req, iteration_loop_position, args, args_url, urls_to_visit):
    if args_url.lower().endswith(EXTENSION_FILE):
        filename = args_url.split('/')[-1]
        filepath = unique_filepath(args.p, filename)
        if not stop_event.is_set():
            with open(filepath, 'wb') as f:
                for chunk in req.iter_content(1024):
                    if not stop_event.is_set():
                        f.write(chunk)
            image_validator(filepath)

    elif iteration_loop_position == "first":
        page = BeautifulSoup(req.content, 'lxml')

        with lock_check_url:
            urls_to_visit[args_url] = page


# This function concern a thread, get request and load images if URL target an image
# First, there is a header to simulate a real browser
# The warning handle line is here to manage error due to xml parsing
# After there is the get request part and check error
# After there is image load part or stocking address for the next time
def get_page(iteration_loop_position, args, args_url, urls_to_visit):

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.90 Safari/537.36'
        }

        warnings.filterwarnings("ignore", category=XMLParsedAsHTMLWarning)

        if not stop_event.is_set():
            r = requests.get(args_url, headers=headers, stream=True, timeout=5)
            r.raise_for_status()
        else:
            return 0

        get_page_actions(r, iteration_loop_position, args, args_url, urls_to_visit)

        return 0

    except requests.Timeout:
        if not stop_event.is_set():
            print(f"{BG_TIMEOUT}{FG_BG_TIMEOUT}[Timeout]  {RESET_COLOR}{FG_TIMEOUT} {args_url}{BG_TIMEOUT} {RESET_COLOR}{FG_TIMEOUT}Impossible to achieve{RESET_COLOR}")
        else:
            pass

    except requests.TooManyRedirects:
        if not stop_event.is_set():
            print(f"{BG_REDIRECT}{FG_BG_TIMEOUT}[Redirect]{RESET_COLOR}{FG_REDIRECT} {args_url}{BG_REDIRECT} {RESET_COLOR}{FG_REDIRECT}Too many redirects{RESET_COLOR}")
        else:
            pass

    except requests.ConnectionError:
        if not stop_event.is_set():
            print(f"{BG_CONNEXION}{FG_BG_CONNEXION}[Connexion]{RESET_COLOR}{FG_CONNEXION} {args_url}{BG_CONNEXION} {RESET_COLOR}{FG_CONNEXION}Connexion error{RESET_COLOR}")
        else:
            pass

    except requests.HTTPError as http_err:
        if not stop_event.is_set():
            print(f"{BG_HTTP}{FG_BG_HTTP}[HTTP {http_err.response.status_code}] {RESET_COLOR}{FG_HTTP} {http_err.response.url}{RESET_COLOR}")
        else:
            pass

    except requests.RequestException as e:
        if not stop_event.is_set():
            print(f"{BG_UNKNOWN}{FG_BG_UNKNOWN}[Unknown]  {RESET_COLOR}{FG_UNKNOWN} {args_url}{BG_UNKNOWN} {RESET_COLOR}{FG_UNKNOWN}{e}{RESET_COLOR}")
        else:
            pass

    with lock_check_url:
        urls_to_visit[args_url] = None

    return 1


# This function concern a thread, extract and built absolute URL to explore
def url_extractor(args, base_url, page, visited_urls):

    for key, value in BALISE_ELEMENT.items():
        for url in page.find_all(key):
            link = url.get(value)
            if link:
                absolute_url = urljoin(base_url,link)
                if absolute_url.startswith("http"):
                    with lock_check_url:
                        visited_urls[absolute_url]


# Join and clear threads
def thread_cleaner(threads):

    for t in threads:
        t.join()

    threads.clear()


# This function send threads for get pages and images
def thread_handler_get_page(position, urls_to_visit, args):

    threads = []

    with lock_check_url:
        temp = urls_to_visit.copy()
    for key, value in temp.items():
        if not stop_event.is_set():
            t = threading.Thread(target=get_page, args=(position, args, key, urls_to_visit))
            threads.append(t)
            t.start()

    thread_cleaner(threads)

    temp.clear()


# This function send threads for build next URLs to get
def thread_handler_url_extractor(urls_to_visit, visited_urls, args):

    threads = []

    for key, value in urls_to_visit.items():
        if value is not None:
            if not stop_event.is_set():
                t = threading.Thread(target=url_extractor, args=(args, key, value, visited_urls))
                threads.append(t)
                t.start()
        else:
            visited_urls[key] = True

    thread_cleaner(threads)


# This function handle next URLs to get
def next_url_handler(urls_to_visit, visited_urls):

    for key, value in urls_to_visit.items():
        visited_urls[key] = True

    urls_to_visit.clear()

    for key, value in visited_urls.items():
        if not value:
            urls_to_visit[key] = None


# It's the main function which handle all url with threads
def page_visiter(args):

    visited_urls = defaultdict(lambda: False)
    urls_to_visit = {args.URL: None}

    recursive_number = 1

    if args.r == True:
        recursive_number += args.l

    for i in range(recursive_number):
        thread_handler_get_page("first", urls_to_visit, args)

        thread_handler_url_extractor(urls_to_visit, visited_urls, args)

        next_url_handler(urls_to_visit, visited_urls)

    thread_handler_get_page("last", urls_to_visit, args)

    if stop_event.is_set():
        return True


# This function handle directory been, right and creation
def directory_handler(args_path):

    if Path(args_path).is_dir() and not os.access(args_path, os.R_OK | os.W_OK):
        raise PermissionError(f"Directory right problem: '{args_path}'")

    os.makedirs(args_path, exist_ok=True)
        

def main():

    try:
        args = parse_args()

        directory_handler(args.p)

        page_visiter(args)

    except PermissionError as e:
        print(f"{BG_PB}{FG_BG_PB}[ERROR]    {RESET_COLOR}{FG_PB} {e}{RESET_COLOR}")

    except Exception as e:
        print(f"{BG_PB}{FG_BG_PB}[ERROR]    {RESET_COLOR}{FG_PB} {e}{RESET_COLOR}")
    

if __name__ == "__main__":
    main()
