#!/usr/bin/env python3

# Standard Python
import os
import sys
import argparse
import time
from urllib.parse import urljoin, urlparse

# Standard Python
import requests
from bs4 import BeautifulSoup


def parse_args():
    parser = argparse.ArgumentParser(description='Spider for downloading images from a URL')

    parser.add_argument('URL', help='The URL of the site to scrape')
    parser.add_argument('-r', action='store_true', \
        help='Recursively downloads the images in a URL received as a parameter.')
    parser.add_argument('-l', metavar='[N]', type=int, default=5, \
        help='Indicates the maximum depth level of the recursive download. If not indicated, it will be 5.')
    parser.add_argument('-p', metavar='[PATH]', default='./data/', \
        help='Indicates the path where the downloaded files will be saved. If not specified, ./data/ will be used.')

    return parser.parse_args()

def get_page(args):

    try:
        headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.90 Safari/537.36'
        }

        r = requests.get(args.URL, headers=headers, timeout=10)
        page = BeautifulSoup(r.text, 'html.parser')
        r.raise_for_status()

        return page

    except requests.Timeout:
        print(f"[Timeout] Impossible to achieve {args.URL}")

    except requests.TooManyRedirects:
        print(f"[Redirect] Too many redirects for {args.URL}")

    except requests.ConnectionError:
        print(f"[Connexion] Connexion error for {args.URL}")

    except requests.HTTPError as http_err:
        print(f"[HTTP {page.status_code}] {args.URL} : {http_err}")

    except requests.RequestException as e:
        print(f"[Erreur inconnue] {args.URL} : {e}")

    return 1

def images_extractor(page, images_set):
    for img in page.find_all('img'):
        src = img.get('src')
        if src:
            images_set.add(src)

    for img in page.find_all('source'):
        srcset = img.get('srcset')
        if srcset:
            images_set.add(srcset)

    for images in images_set:
        print(images)


def main():
    images_set = set()
    #visited_url_set = set()

    args = parse_args()
    page = get_page(args)
    images_extractor(page, images_set)
    

if __name__ == "__main__":
    main()