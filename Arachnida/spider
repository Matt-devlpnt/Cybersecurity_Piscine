#!/usr/bin/env python3

# Roadmap
# Tester une page avec uniquement du texte
# Faire un ping pour tester les url
# Faire attention a ce que les fichier aillent bien dans le bon dossier avec option -p

# Standard Python
import os
import sys
import argparse
import time
import threading
from urllib.parse import urljoin, urlparse
from collections import defaultdict

# Standard Python
import requests
from bs4 import BeautifulSoup

# Globale variable
EXTENSION_FILE = ('.jpg', '.jpeg', '.png', '.gif', '.bmp')

BALISE_ELEMENT = {
    'a': 'href',
    'link': 'href',
    'iframe': 'src',
    'img': 'src',
    'source': 'srcset'
}

lock_check_url = threading.Lock()


def parse_args():
    parser = argparse.ArgumentParser(description='Spider for downloading images from a URL')

    parser.add_argument('URL', help='The URL of the site to scrape')
    parser.add_argument('-r', action='store_true', \
        help='Recursively downloads the images in a URL received as a parameter.')
    parser.add_argument('-l', metavar='[N]', type=int, default=5, \
        help='Indicates the maximum depth level of the recursive download. If not indicated, it will be 5.')
    parser.add_argument('-p', metavar='[PATH]', default='./data/', \
        help='Indicates the path where the downloaded files will be saved. If not specified, ./data/ will be used.')

    return parser.parse_args()


def get_page(args):

    try:
        headers = {
        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.90 Safari/537.36'
        }

        r = requests.get(args.URL, headers=headers, timeout=5)
        page = BeautifulSoup(r.text, 'html.parser')
        r.raise_for_status()

        return page

    except requests.Timeout:
        print(f"[Timeout] Impossible to achieve {args.URL}")

    except requests.TooManyRedirects:
        print(f"[Redirect] Too many redirects for {args.URL}")

    except requests.ConnectionError:
        print(f"[Connexion] Connexion error for {args.URL}")

    except requests.HTTPError as http_err:
        print(f"[HTTP {page.status_code}] {args.URL} : {http_err}")

    except requests.RequestException as e:
        print(f"[Erreur inconnue] {args.URL} : {e}")

    return 1


def accessibility_page_checker(url, visited_urls):
    try:
        r = requests.head(url, allow_redirects=True, timeout=5)

    except requests.RequestException:
        with lock_check_url:
            del visited_urls[url]


#def images_extractor(page, base_url, images_set):
#    for key, value in BALISE_ELEMENT.items():
#        for url in page.find_all(key):
#            link = url.get(value)
#            if link:
#                absolute_url = urljoin(base_url,link)
#                if accessibility_page_checker(absolute_url):
#                    if absolute_url.lower().endswith(EXTENSION_FILE):
#                        images_set.add(absolute_url)


def url_extractor(page, base_url, visited_urls):
    threads = []

    for key, value in BALISE_ELEMENT.items():
        for url in page.find_all(key):
            link = url.get(value)
            if link:
                absolute_url = urljoin(base_url,link)
                visited_urls[absolute_url]

    for key, value in visited_urls.items():
        t = threading.Thread(target=accessibility_page_checker, args=(key, visited_urls))
        threads.append(t)
        t.start()

    for t in threads:
        t.join()
        

def main():
    import_images = defaultdict(lambda: False)
    visited_urls = defaultdict(lambda: False)

    args = parse_args()
    page = get_page(args)
    url_extractor(page, args.URL, visited_urls)
    #images_extractor(page, args.URL, images_set)

    print("URL:")
    for key, value in visited_urls.items():
        print(key + " " + str(value))

    print("")

    print("Images:")
    for key, value in import_images.items():
        print(key + " " + str(value))
    

if __name__ == "__main__":
    main()